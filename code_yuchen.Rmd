---
title: "code"
author: "Qi Yuchen, yq2279"
date: "2020/5/16"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(mgcv)
library(randomForest)
library(tidyverse)
library(ggplot2)
library(glmnet)
library(corrplot)
library(patchwork)
library(AppliedPredictiveModeling)
library(skimr)
library(rpart.plot)
library(gbm)
library(plotmo)
library(pdp)
library(pROC)
library(lime)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Exploratory analysis

## data imput

First, we identify the missing values in the dataset. As is shown in the table below, there is no variable containing missing data.

```{r}
df.raw = read_csv("winequality-red.csv")
# check NA data
df.na = is.na(df.raw)
var.na = colSums(df.na)
var.na
```

Then we check the response variable `quality`. As the distribution is not balanced and we are intereted in whether the wine is good or not, we divide it into two classes, poor (quality < 5.5) or good (quality > 5.5). After cleaning the names of the variables, we divide the data into traning and testing data.

```{r}
df.raw %>% 
  ggplot(aes(x = quality)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(3,8,1))

df = df.raw %>% 
  janitor::clean_names() %>% 
  mutate(quality = as.factor(quality)) %>% 
  mutate(quality = recode(quality, '3' = "poor",'4' = "poor",'5' = "poor",'6' = "good",'7' =  "good",'8' = "good"))

set.seed(1)
rowTrain <- createDataPartition(y = df$quality,
                                p = 2/3,
                                list = FALSE)
df.train = df[rowTrain,]
x = model.matrix(quality~., df.train)[,-1]
y = df.train$quality

df.test = df[-rowTrain,]

summary(df$quality) # 6 levels
```


## Correlations 

11 predictors are all numerical variables. There is no strong correlation (>0.7) between them. 

```{r}
var.numerical = df.train %>% dplyr::select_if(is.numeric) %>% as.matrix()
var.cor = cor(var.numerical)

corrplot(cor(var.numerical), method = "square", type = "full")

which((var.cor > 0.7 & var.cor < 1), arr.ind = TRUE)
```

## Scatter plot 

```{r}
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x, 
            y,
            scales = list(x=list(relation="free"), 
                          y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```


# Models

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```


## LDA and QDA

```{r}
set.seed(1)
model.lda <- train(quality~., df, 
                   subset = rowTrain,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
set.seed(1)
model.qda <- train(quality~., df, 
                   subset = rowTrain,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

```


## KNN

```{r}
set.seed(1)
knn.fit <- train(quality~., df, 
                   subset = rowTrain,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)

ggplot(knn.fit)
```


## Classification tree

```{r}
set.seed(1)
rpart.fit <- train(quality~., df, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-8,-5, len = 20))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

## Random forests

```{r}
rf.grid <- expand.grid(mtry = 1:6,
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(1)
rf.fit <- train(quality~., df, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = df[-rowTrain,], type = "prob")[,1]
roc.rf <- roc(df$quality[-rowTrain], rf.pred)
plot(roc.rf)
auc <- roc.rf$auc[1]
modelNames <- "rf"
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```

## Boosting

```{r}
gbmA.grid <- expand.grid(n.trees = c(3000,4000,5000),
                        interaction.depth = 19:24,
                        shrinkage = c(0.003,0.004, 0.005),
                        n.minobsinnode = 1)
set.seed(1)
# Adaboost loss function
gbmA.fit <- train(quality~., df, 
                 subset = rowTrain, 
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)
gbmA.fit$bestTune
```

 
 
n.trees
<dbl>
interaction.depth
<int>
shrinkage
<dbl>
n.minobsinnode
<dbl>
51	5000	23	0.005	

## performance

```{r}
resamp <- resamples(list(rf = rf.fit, 
                         knn = knn.fit,
                         lda = model.lda,
                         qda = model.qda,
                         rpart = rpart.fit,
                         boost = gbmA.fit))
summary(resamp)
```


